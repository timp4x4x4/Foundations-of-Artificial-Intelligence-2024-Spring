# FAI 期末專案：使用 Q-Learning 開發撲克牌 AI 玩家
**作者**: 陳奕廷 (B11705051)  
**日期**: 2024/06/15  

---

## 簡介
本專案旨在利用 **Q-Learning** 開發一個撲克牌遊戲的 AI 玩家。  
目的是探索並比較不同的方法，以確定 AI 的最佳策略。

---

## 方法

### 1. 方法一：基於牌組狀態的基本 Q-Learning
- **描述**:  
  使用牌組狀態（手牌和公共牌）作為狀態表示。
- **狀態表示**:  
  將手牌和公共牌的狀態透過字串串接的方式表示。
- **獎勵**:  
  根據最終牌組的強度給予獎勵。
- **結果**:  
  此方法的狀態空間過大，導致學習速度緩慢且效率低下，並需要大量空間儲存所有組合，通用性不高。

---

### 2. 方法二：基於簡化狀態（勝率）的 Q-Learning
- **描述**:  
  為了減少狀態空間，改用計算出的勝率來表示狀態。
- **狀態表示**:  
  狀態以格式 `xx | xx | xx` 表示，其中每一部分代表各回合計算出的勝率。每回合的勝率會依序加入字串中，提供遊戲過程中勝率序列的累積表示。
- **獎勵**:  
  根據最終牌組的強度給予獎勵。
- **結果**:  
  此方法大幅減少了空間需求，提高了學習效率，並取得比方法一更佳的效能。

---

### 3. 方法三：結合領域知識的 Q-Learning
- **描述**:  
  在前兩種方法中，模型使用隨機行動來探索更多選項，但效果不佳。因此，我加入了一些領域知識，避免出現例如在勝率為 0 時進行加注等不良行為。此外，模型的決策將基於勝率進行，而非隨機選擇。
- **狀態表示**:  
  與方法二相同。
- **行動空間**:  
  `['fold', 'call', 'raise']`
- **獎勵**:  
  根據最終牌組的強度與遊戲結果給予獎勵。
- **結果**:  
  此方法顯著改進了效能。雖然方法二仍無法穩定擊敗較弱的基線模型，但方法三有效縮小了差距，並顯示出更穩定的表現。

---

## 參數設定 (Hyperparameters)

### 1. 使用的參數
- 學習率 (α): 0.05  
- 折扣因子 (γ): 0.9  
- 探索率 (ϵ): 1.0（初始值）  
- 探索衰減率 (ϵ_decay): 0.99  
- 獎勵結構:  
  根據牌組強度設置獎勵。

### 2. 最終配置
最終選擇的方法三，結合領域知識的 Q-Learning。  
此配置在學習效率與效能之間取得良好平衡，並能有效矯正模型的不良行為。

---

## 方法比較

### 1. 比較指標
- **學習效率**: AI 提升效能的速度。  
- **效能**: 勝率與決策穩定性。  
- **狀態空間大小**: 狀態表示的複雜性。

### 2. 結果
- **方法一**: 高狀態空間複雜度，學習速度慢，效能較低。  
- **方法二**: 狀態空間減少，學習速度更快，效能有所提升。  
- **方法三**: 探索與利用平衡，學習效率高，效能穩定且更佳。

---

## 討論
在上述方法中，方法三展現了最佳效能。然而，由於計算資源的限制，可能導致方法三達到的是局部最佳而非全域最佳。  
這表明，若採用更高效的訓練方法或更強大的計算資源，方法二或許能訓練出一個更大的模型，無需依賴領域知識，也能更全面地學習遊戲動態。

---

## 結論
本專案透過 Q-Learning 開發撲克牌遊戲的 AI 玩家，測試了三種主要方法，各自改進了 AI 的狀態表示與決策過程。  
- 方法一使用直接的牌組狀態，導致過大的狀態空間與較慢的學習速度。  
- 方法二簡化了狀態表示為勝率，提升了效率與效能。  
- 方法三結合領域知識進一步優化決策，顯著提高了效能與穩定性。  

比較分析結果顯示，方法三最具成效，但其局限性表明進一步的改進空間，如調整獎勵結構或探索更先進的學習算法。

本專案展示了 Q-Learning 在開發複雜遊戲 AI 策略上的有效性，為遊戲與決策領域的 AI 技術發展提供了寶貴的啟示。
